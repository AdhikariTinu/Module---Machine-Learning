{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d241f3d-c26c-47e0-bd47-42b538094c11",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7169c39-054c-440c-a1b0-6f002a329269",
   "metadata": {},
   "source": [
    "## Q.1. What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918649bc-19e3-4aa5-bf6e-d403596ab6da",
   "metadata": {},
   "source": [
    "--> A model parameter is a configuration variable that is internal to the model and whose value can be estimated or learned from the given training data. Parameters define the skill of the model on a specific problem and are not set manually by the practitioner. Examples include the coefficients in linear regression or the weights and biases in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d52880-5a2d-4953-b0cb-ba16f35cd144",
   "metadata": {},
   "source": [
    "## Q.2. What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00380a-9cf7-4ee8-904a-dff6212dcd74",
   "metadata": {},
   "source": [
    "--> Correlation is a statistical measure that describes the degree and direction of the linear relationship between two or more variables. It quantifies the extent to which changes in one variable are associated with changes in another. The relationship is expressed by a correlation coefficient (r) which ranges from -1 to +1.\n",
    "\n",
    "--> Negative correlation (or inverse correlation) means that as one variable increases, the other variable generally decreases, and vice versa. The correlation coefficient for a negative correlation is a value below 0, with -1 indicating a perfect negative relationship. An example is the relationship between price and demand: as the price of a commodity increases, its demand tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67489ae4-03b3-4fb8-a18b-42f04740ddaa",
   "metadata": {},
   "source": [
    "## Q.3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1308e-b45d-4480-8f9d-a67c01b9d54e",
   "metadata": {},
   "source": [
    "--> Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on building systems that can learn from data and improve with experience without being explicitly programmed.\n",
    "\n",
    "--> The main components in machine learning typically involve:\n",
    "\n",
    "> Data:\n",
    ">> High-quality, relevant data is the foundation for training ML models.\n",
    "\n",
    "> Algorithm:\n",
    ">> A set of rules and procedures used to solve a specific problem or perform a task.\n",
    "\n",
    "> Model:\n",
    ">> The output or result of applying an algorithm to a dataset after training, which is used to make predictions.\n",
    "\n",
    "> Loss Function/Objective:\n",
    ">> A mathematical function that measures the error between the model's predictions and the actual values. The goal is to minimize this loss during training.\n",
    "\n",
    "> Optimization:\n",
    ">> The process of adjusting the model's internal parameters (weights and biases) to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079392d-045f-472e-b1bd-9ba475964ad0",
   "metadata": {},
   "source": [
    "## Q.4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c2a7d-1eff-4706-9c28-f753af9c6d3d",
   "metadata": {},
   "source": [
    "--> The loss value is a numerical metric that quantifies the difference (error) between a model's predicted values and the actual true values (ground truth). A lower loss value indicates that the model's predictions are closer to the actual values, meaning the model is performing well. During training, the goal is to minimize this loss value, bringing it as close to zero as possible. A high loss value, conversely, indicates poor model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe5602-e86b-4288-b23c-2050d37d58f4",
   "metadata": {},
   "source": [
    "## Q.5. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd662a-5a17-4c0d-ab1a-356e27eae788",
   "metadata": {},
   "source": [
    "--> Continuous variables are numerical data that can take any value within a given range, including infinite intermediate values. Examples include height, weight, temperature, or time, which are typically measured.\n",
    "\n",
    "--> Categorical variables represent distinct groups or categories and are descriptive rather than numerical. Examples include hair color, dog breed, or education level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d27df9-79c2-4e90-ad64-407a28c95b05",
   "metadata": {},
   "source": [
    "## Q.6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33925cc-b1ca-43b4-8f9a-d084012433f7",
   "metadata": {},
   "source": [
    "--> Categorical variables must be converted to a numerical format because most machine learning algorithms require numerical input. Common techniques include: \n",
    "\n",
    "> One-Hot Encoding:\n",
    ">> Creates a new binary (0 or 1) column for each category present in the original feature. It is suitable for nominal data where no inherent order exists between categories.\n",
    "\n",
    ">Label Encoding:\n",
    ">> Assigns a unique integer value to each category. This is best used for ordinal data where the order of categories matters (e.g., \"small\", \"medium\", \"large\" might be encoded as 0, 1, 2). Using it for nominal data can imply an order that does not exist.\n",
    "\n",
    ">Frequency Encoding:\n",
    ">> Replaces each category with its frequency or count in the dataset.\n",
    "\n",
    ">Target Encoding:\n",
    ">> Replaces a category with the mean of the target variable for that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0795-2c74-41d3-8b88-2af5b38941e8",
   "metadata": {},
   "source": [
    "## Q.7. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51235b-4dd8-4677-a6bd-000050eb3e1a",
   "metadata": {},
   "source": [
    "--> Training refers to the process of using a large, labeled portion of data to teach a machine learning model to recognize patterns and adjust its internal parameters (weights and biases).\n",
    "\n",
    "--> Testing involves using a separate, unseen subset of the data to evaluate the performance and generalization ability of the trained model. This step ensures the model hasn't simply memorized the training data (overfitting) and can make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205ed9b-e917-4ede-ba91-20630e0d932e",
   "metadata": {},
   "source": [
    "## Q.8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4ef64-6c77-4e1f-9c9b-a444d6f6d47d",
   "metadata": {},
   "source": [
    "--> The sklearn.preprocessing package in the scikit-learn Python library provides several utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for downstream machine learning estimators. These tools are essential for data cleaning and preparation, and include functions for standardization, scaling features to a range, normalization, binarization, encoding categorical features, and imputing missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ce12-3d40-4ab0-99ff-b8774739579d",
   "metadata": {},
   "source": [
    "## Q.9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef48a69-c8ad-4633-9684-44369399f407",
   "metadata": {},
   "source": [
    "--> A test set is a dataset that is independent of the data used for training and validation. It is used only at the very end of the model development process to provide an unbiased evaluation of the final model's performance on unseen, real-world data. It helps assess how well the model generalizes and is crucial for detecting issues like overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60169c71-2573-49be-8400-3e608d79ad82",
   "metadata": {},
   "source": [
    "## Q.10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7490470-0631-4443-a494-b599d6e4e2b4",
   "metadata": {},
   "source": [
    "--> The data is typically split using the train_test_split() function from the sklearn.model_selection module in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a1b1f-de09-4d8e-8fc2-c513e863b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d1c9d-d0d3-4b42-ba8b-1580aa91b9c1",
   "metadata": {},
   "source": [
    "--> This function takes the feature matrix (x) and target vector (y) as input. Key parameters include test_size (the proportion of data for the test set, commonly 0.2 or 0.3) and random_state (ensures the split is the same every time the code runs for reproducibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee33ad0-8e27-4e1f-96c9-fdd4b3073ca4",
   "metadata": {},
   "source": [
    "--> A structured approach to a machine learning problem typically involves several key steps:\n",
    "\n",
    ">Define the problem:\n",
    ">> Understand the goal and the desired outcome (e.g., classification, regression).\n",
    "\n",
    ">Gather the data:\n",
    ">> Collect relevant and high-quality data from various sources.\n",
    "\n",
    ">Explore and visualize data (EDA):\n",
    ">> Analyze the data to find patterns, check for missing values or outliers, and understand relationships between variables.\n",
    "\n",
    ">Prepare the data:\n",
    ">> Clean the data, handle missing values, encode categorical variables, and scale numerical features (data preprocessing).\n",
    "\n",
    ">Select a model and train it:\n",
    ">>Choose an appropriate algorithm and fit it to the training data.\n",
    "\n",
    ">>Fine-tune the model:\n",
    ">>>Optimize hyperparameters and use techniques like cross-validation to improve performance.\n",
    "\n",
    ">>Evaluate the model:\n",
    ">>>Assess the final model's performance on the test set using relevant metrics.\n",
    "\n",
    ">>Deploy and monitor:\n",
    ">>>Launch the model in a real-world environment and monitor its performance over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1b331-9a3e-478d-ae08-7b6bb6a61e26",
   "metadata": {},
   "source": [
    "## Q.11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46799b05-2f5c-4666-99e2-dbe3dcc151ba",
   "metadata": {},
   "source": [
    "--> Exploratory Data Analysis (EDA) is crucial because it helps to:\n",
    "\n",
    ">Understand the data:\n",
    ">> Provides insight into the structure, variables, and potential issues within the dataset.\n",
    "\n",
    ">Identify issues:\n",
    ">> Helps in detecting missing values, outliers, or errors in the data that could negatively impact model performance.\n",
    "\n",
    ">Formulate hypotheses:\n",
    ">> Allows practitioners to uncover patterns and relationships that can guide model selection and feature engineering decisions.\n",
    "\n",
    ">Verify assumptions:\n",
    ">> Checks if the data meets the assumptions required by certain statistical procedures or machine learning algorithms.\n",
    "\n",
    ">Prepare data effectively:\n",
    ">> Informs the necessary data cleaning and preprocessing steps needed before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a30d82-eee1-410f-802c-c50551a2d40b",
   "metadata": {},
   "source": [
    "## Q.12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe0365-cf8c-42fc-a62e-1b7b0eedd07d",
   "metadata": {},
   "source": [
    "--> Correlation is a statistical measure that describes the degree and direction of the linear relationship between two or more variables. It quantifies the extent to which changes in one variable are associated with changes in another. The relationship is expressed by a correlation coefficient (r) which ranges from -1 to +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1a731-c2fd-45e5-9eaa-ff82421cb948",
   "metadata": {},
   "source": [
    "## Q.13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9cd1f-10b0-4a8c-befb-c9912c217a14",
   "metadata": {},
   "source": [
    "--> Negative correlation (or inverse correlation) means that as one variable increases, the other variable generally decreases, and vice versa. The correlation coefficient for a negative correlation is a value below 0, with -1 indicating a perfect negative relationship. An example is the relationship between price and demand: as the price of a commodity increases, its demand tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13ba1c-5ecc-4b92-955d-d32756cd8067",
   "metadata": {},
   "source": [
    "## Q.14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fa509-c0a8-4b63-a7a8-1919f842c681",
   "metadata": {},
   "source": [
    "--> Correlation between variables in Python can be found using functions from libraries like pandas and numpy.\n",
    "\n",
    ">Pandas:\n",
    ">>The .corr() method on a DataFrame computes the correlation matrix, showing the correlation coefficient between all pairs of columns.\n",
    "\n",
    ">Numpy:\n",
    ">>The numpy.corrcoef() function can be used to calculate the Pearson product-moment correlation coefficient for two variables.\n",
    "\n",
    ">Visualization:\n",
    ">>Libraries like matplotlib or seaborn can be used to create scatter plots or heatmaps of the correlation matrix, which visually represent the relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e7850-126b-4180-9f33-9cf14d4dc284",
   "metadata": {},
   "source": [
    "## Q.15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784581c3-8631-4c07-bc57-ac1103911ee2",
   "metadata": {},
   "source": [
    "--> Causation indicates that one event is the direct result of the occurrence of another event; a true cause-and-effect relationship. \n",
    "\n",
    "Correlation means that two or more variables are statistically related or associated, but it does not automatically imply that one causes the other. \n",
    "\n",
    "Example: Sales of ice cream and sales of sunscreen are highly correlated because both tend to increase during the summer months. However, increased ice cream sales do not cause increased sunscreen sales. The underlying cause for both is a third variable: hotter weather/season. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21daf5-1ec6-4c31-b87e-c779b926b79e",
   "metadata": {},
   "source": [
    "## Q.16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5e35e-8684-4ecc-8e57-76509ec148fa",
   "metadata": {},
   "source": [
    "--> An optimizer is an algorithm or method used to minimize the error (loss function) of a machine learning model by iteratively adjusting the model's learnable parameters (weights and biases). \n",
    "\n",
    "Types of optimizers include:\n",
    "\n",
    ">Gradient Descent (Batch Gradient Descent):\n",
    ">>Calculates the gradient of the loss function using the entire training dataset to update parameters in one step. It is slow and computationally expensive for large datasets.\n",
    "\n",
    ">Stochastic Gradient Descent (SGD):\n",
    ">>Updates the model parameters one by one for each training example. It requires less memory and is faster for large datasets, though the updates can be noisy.\n",
    "\n",
    ">Mini-Batch Gradient Descent:\n",
    ">>Splits the training data into small batches and performs an update for each batch, balancing the robustness of SGD and the efficiency of Batch Gradient Descent.\n",
    "\n",
    ">Adam (Adaptive Moment Estimation):\n",
    ">>One of the most popular optimizers, it computes adaptive learning rates for each parameter by storing decaying averages of past gradients and squared gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4912f28-118a-4419-b105-2023c60886ad",
   "metadata": {},
   "source": [
    "## Q.17. What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e72a11-fccc-4507-b774-22e9e0acae1e",
   "metadata": {},
   "source": [
    "--> sklearn.linear_model is a module within the scikit-learn library that contains a variety of functions for performing machine learning using linear models. Linear models assume that the target variable can be predicted using a linear function of the input features. This module includes algorithms for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2ea4d-6e8d-4a08-b25e-81963dc58520",
   "metadata": {},
   "source": [
    "## Q.18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b68fc-a684-4a4e-aef9-588ce59f57e6",
   "metadata": {},
   "source": [
    "--> The model.fit() method is used to train a machine learning model in scikit-learn. It adjusts the internal parameters of the model based on the provided data to learn underlying patterns. \n",
    "\n",
    "The two required arguments are:\n",
    "\n",
    "x: The feature matrix (input data), where each row represents a sample and each column represents a feature.\n",
    "\n",
    "y: The target vector (labels or target values) corresponding to the samples in x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cc55c-95e4-4941-8ce9-1918debb7ae2",
   "metadata": {},
   "source": [
    "## Q.19. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e996a-626c-4407-ae4b-e5d87621d124",
   "metadata": {},
   "source": [
    "--> The model.predict() method is used to make predictions on new, unseen data using the patterns learned during the fit() process. \n",
    "\n",
    "The primary argument that must be given is:\n",
    "\n",
    "X_test: The feature matrix of the new input data for which you want predictions. This data should be in the same format and have the same features as the data used for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1a5f7-e448-463c-8112-3b3093c80a50",
   "metadata": {},
   "source": [
    "## Q.20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b5f9d-a4d6-4b95-a725-3f2723bc9296",
   "metadata": {},
   "source": [
    "--> Continuous variables are numerical data that can take any value within a given range, including infinite intermediate values. Examples include height, weight, temperature, or time, which are typically measured.\n",
    "\n",
    "--> Categorical variables represent distinct groups or categories and are descriptive rather than numerical. Examples include hair color, dog breed, or education level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b091b6-ec10-48aa-b0c4-4835bf79a0a2",
   "metadata": {},
   "source": [
    "## Q.21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f9672-e61b-47e6-a7c3-e502ea0fb83d",
   "metadata": {},
   "source": [
    "--> Feature scaling is a data preprocessing technique used to standardize the range of independent features or variables in a dataset. \n",
    "\n",
    "It helps in machine learning by:\n",
    "\n",
    ">Ensuring fairness:\n",
    ">>Prevents features with larger magnitudes from dominating the learning process or objective function.\n",
    "\n",
    ">Improving performance:\n",
    ">>Many algorithms, such as gradient descent, k-nearest neighbors (KNN), and support vector machines (SVM), perform much better or converge faster when features are on a similar scale.\n",
    "\n",
    ">Optimizing algorithms:\n",
    ">>Helps optimization algorithms work more efficiently by avoiding issues caused by different scales of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5d136-a75f-4128-9628-3bfcee5c9bc8",
   "metadata": {},
   "source": [
    "## Q.22.How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e005f-bd41-4db3-bcf2-15afcfa54830",
   "metadata": {},
   "source": [
    "--> Scaling in Python is commonly performed using classes from the sklearn.preprocessing module.\n",
    "\n",
    ">Standardization (StandardScaler):\n",
    ">>Transforms data to have a mean of zero and a unit variance.\n",
    "\n",
    ">Normalization to a range (MinMaxScaler):\n",
    ">>Rescales features to a fixed range, typically between (0, 1). \n",
    "\n",
    "These classes use fit() on the training data to learn the scaling parameters (mean, standard deviation, min/max values) and then transform() to apply the scaling to both training and test data consistently. The fit_transform() method can be used as a shortcut on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e8903-1503-45ef-885f-464af2b9f66f",
   "metadata": {},
   "source": [
    "## Q.23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffc342-de77-4d4b-8713-daac78f614dd",
   "metadata": {},
   "source": [
    "--> The sklearn.preprocessing package in the scikit-learn Python library provides several utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for downstream machine learning estimators. These tools are essential for data cleaning and preparation, and include functions for standardization, scaling features to a range, normalization, binarization, encoding categorical features, and imputing missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71846d9-1e9c-4213-9a2c-a8e2e4c7d462",
   "metadata": {},
   "source": [
    "## Q.24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada19a8-98a2-421d-9de4-8dde6952f582",
   "metadata": {},
   "source": [
    "--> The data is typically split using the train_test_split() function from the sklearn.model_selection module in the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6c318-169c-4da2-a7c5-db471bb84aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b65a9-835d-4719-81f5-9649bfb60861",
   "metadata": {},
   "source": [
    "--> This function takes the feature matrix (x) and target vector (y) as input. Key parameters include test_size (the proportion of data for the test set, commonly 0.2 or 0.3) and random_state (ensures the split is the same every time the code runs for reproducibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3fd7f-d490-4615-b230-6f0beaec95c8",
   "metadata": {},
   "source": [
    "--> A structured approach to a machine learning problem typically involves several key steps:\n",
    "\n",
    ">Define the problem:\n",
    ">> Understand the goal and the desired outcome (e.g., classification, regression).\n",
    "\n",
    ">Gather the data:\n",
    ">> Collect relevant and high-quality data from various sources.\n",
    "\n",
    ">Explore and visualize data (EDA):\n",
    ">> Analyze the data to find patterns, check for missing values or outliers, and understand relationships between variables.\n",
    "\n",
    ">Prepare the data:\n",
    ">> Clean the data, handle missing values, encode categorical variables, and scale numerical features (data preprocessing).\n",
    "\n",
    ">Select a model and train it:\n",
    ">>Choose an appropriate algorithm and fit it to the training data.\n",
    "\n",
    ">>Fine-tune the model:\n",
    ">>>Optimize hyperparameters and use techniques like cross-validation to improve performance.\n",
    "\n",
    ">>Evaluate the model:\n",
    ">>>Assess the final model's performance on the test set using relevant metrics.\n",
    "\n",
    ">>Deploy and monitor:\n",
    ">>>Launch the model in a real-world environment and monitor its performance over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db201e-55fd-4bf4-b498-e4c21b1ad6f2",
   "metadata": {},
   "source": [
    "## Q.25. Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c11f32-0208-4627-ae42-7e14317064b2",
   "metadata": {},
   "source": [
    "--> Data encoding is the process of converting data from one form to another, specifically in machine learning, it refers to converting categorical data (labels or categories) into a numerical format that algorithms can understand and process. Common methods include One-Hot Encoding and Label Encoding, each suitable depending on whether the categorical data is nominal (unordered) or ordinal (ordered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9594e-9de9-4865-8b52-24a787565117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
